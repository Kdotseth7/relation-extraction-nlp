{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcN8GECJkRZD"
   },
   "outputs": [],
   "source": [
    "!mv ./kaggle.json /root/.kaggle/\n",
    "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "!unzip ./imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OVvz-8JoWRs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO-vlmj4sGZF"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmTMCfBJof3r"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./IMDB Dataset.csv')\n",
    "train_data, val_data = train_test_split(df, test_size=0.2)\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVNEiexrol57"
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIgwGP0xvUVb"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer('this is a test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rB7VEKt1ODh"
   },
   "outputs": [],
   "source": [
    "vocab_size = 8_000\n",
    "all_tokens = []\n",
    "for review in tqdm(train_data['review']):\n",
    "  tokens = tokenizer(review)\n",
    "  all_tokens.extend([i.text for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdMvJ3xs8AmV"
   },
   "outputs": [],
   "source": [
    "count = Counter(all_tokens)\n",
    "tokens, counts = zip(*count.most_common(vocab_size))\n",
    "vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "vocab['<unk>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OleQxm_P8wvJ"
   },
   "outputs": [],
   "source": [
    "print(vocab['<unk>'])\n",
    "print(vocab['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeorEKL1pO9j"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "  def __init__(self, data: pd.DataFrame, vocab):\n",
    "    self.data = data\n",
    "    self.vocab = vocab\n",
    "    self.default = self.vocab['<unk>']\n",
    "\n",
    "  def tokenize(self, text: str):\n",
    "    return [i.text for i in tokenizer(text)]\n",
    "\n",
    "  def encode_tokens(self, tokens):\n",
    "    encoded = [self.vocab.get(token, self.default) for token in tokens]\n",
    "    return torch.tensor(encoded, device=device)\n",
    "\n",
    "  def encode_label(self, label: str):\n",
    "    return torch.tensor(0, device=device) if label == 'negative' else torch.tensor(1, device=device)\n",
    "  \n",
    "  def __getitem__(self, n: int):\n",
    "    review = self.data['review'].iloc[n]\n",
    "    sentiment = self.data['sentiment'].iloc[n]\n",
    "    return self.encode_tokens(self.tokenize(review)), self.encode_label(sentiment)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTam_QwguXgU"
   },
   "outputs": [],
   "source": [
    "train_ds = IMDBDataset(train_data, vocab)\n",
    "val_ds = IMDBDataset(val_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77OMpyO4uaJB"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK2k1sMMxUgp"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_tokens, emb_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "    self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: Tensor([[0, 1, 2, 5, 100, 3, 6]]), shape [B, seq_len]\n",
    "    # embedding.weight:\n",
    "    # 0:       [ 0.3, 0.5, ..., 0.7]\n",
    "    #          ...\n",
    "    # n_token: [ 1.0, 0.8, ..., 0.8]\n",
    "    # \n",
    "    # embedded = embedding(0) + embedding(1) + ... + embedding(6)\n",
    "    embedded = self.embedding(x)\n",
    "    # embedded: Tensor([[0.4, 0.2, ..., -0.9]]), shape [B, emb_dim]\n",
    "    hidden1 = self.fc1(embedded)\n",
    "    hidden2 = self.relu(hidden1)\n",
    "    hidden3 = self.fc2(hidden2)\n",
    "    return hidden3.sum(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-y-tE1X1FIE"
   },
   "outputs": [],
   "source": [
    "model = MLP(vocab_size + 1, 100, 200, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m9JabMa2AUe"
   },
   "outputs": [],
   "source": [
    "def train(loader, model, optimizer, loss_fn):\n",
    "  model.train()\n",
    "  losses = []\n",
    "  pbar = tqdm(loader)\n",
    "  for x, y in pbar:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run the model on the input\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    pbar.set_postfix({'loss': loss.item()})\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()  # calculate gradients for w/b\n",
    "    optimizer.step()  # update weights according to optimizer rules\n",
    "  return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "def evaluate(loader, model, loss_fn, score_fn):\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  labels = []\n",
    "  for x, y in tqdm(loader):\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    predictions.append(pred.numpy())\n",
    "    labels.append(y.numpy())\n",
    "  score = score_fn(labels, predictions)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCY0lQYF5oHV"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "score_fn = accuracy_score\n",
    "n_epochs = 3\n",
    "best_acc = 0\n",
    "for epoch in range(n_epochs):\n",
    "  avg_loss = train(train_loader, model, optimizer, loss_fn)\n",
    "  print('train loss: ', avg_loss)\n",
    "  accuracy = evaluate(val_loader, model, loss_fn, score_fn)\n",
    "  print('val accuracy: ', accuracy)\n",
    "  if accuracy > best_acc and accuracy > 0.7:\n",
    "    torch.save(model.state_dict(), f'best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKvgXjIK6qtp"
   },
   "outputs": [],
   "source": [
    "s1 = torch.randint(0, 10, (1, 6))\n",
    "pad = torch.zeros(size=(1, 4)) - 1\n",
    "s3 = torch.cat([s1, pad], dim=1)\n",
    "s2 = torch.randint(0, 10, (1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAMmr0uaDxXL"
   },
   "outputs": [],
   "source": [
    "torch.cat([s1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Nfkt7arD1iO"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zETITy4tFXvf"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfmi7OZlFaUP"
   },
   "outputs": [],
   "source": [
    "mlb.fit([['apple', 'banana', 'orange']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP5_ScOOFcan"
   },
   "outputs": [],
   "source": [
    "mlb.transform([['apple', 'banana']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtqZPvRQFfQa"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MSEMsgFFp09"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOgg2VnlwIsFgdABWOgrS2Y",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
