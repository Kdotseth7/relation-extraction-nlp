{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b19233",
   "metadata": {
    "id": "50b19233"
   },
   "source": [
    "# Homework 1 Helper\n",
    "\n",
    "Hi everyone! Diving head-first into pytorch is challenging, and there are a lot of different parts at play. Hopefully this notebook can help you a bit with the major challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa9f08",
   "metadata": {
    "id": "f2aa9f08"
   },
   "source": [
    "### Data\n",
    "\n",
    "The first major hurdle is getting your data processed and ready to consume by the model. Your task is multi-label classification: each example can have 0, 1, or more correct labels, and your (text, labels) pairs have to reflect that.\n",
    "\n",
    "This class should take you through the general structure of a `Dataset` object. I've marked a whole bunch of `TODO`s in the comments, as well as some comments as a refresher.\n",
    "\n",
    "**Be sure to understand what the code you're writing is doing  what it's for!** This is absolutely critical. This  structure is nearly the same for almost every neural network you'll write in pytorch (including Homework 2-4), with some variations depending on the task/dataset and author of the code. The earlier you understand it and the more practice you get, the better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff899f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2216,
     "status": "ok",
     "timestamp": 1664552292627,
     "user": {
      "displayName": "Nilay Patel",
      "userId": "07518589520862055822"
     },
     "user_tz": 420
    },
    "id": "9ff899f7"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda29448",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664553092366,
     "user": {
      "displayName": "Nilay Patel",
      "userId": "07518589520862055822"
     },
     "user_tz": 420
    },
    "id": "dda29448"
   },
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,  # Path to the training data csv\n",
    "        vocab_size: int = 1_000,  # How many tokens to include in the vocabulary. Feel free to adjust this!\n",
    "    ):\n",
    "        # Read the csv using pandas read_csv function.\n",
    "        self.data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "        # the given column names are long, so I often rename them for simplicity.\n",
    "        self.data.columns = [\"text\", \"labels\"]\n",
    "\n",
    "        # There's a problem with this data...some of the rows have a label called 'none', and others\n",
    "        # are just empty. These are both referring to the same condition, so lets replace the 'none'\n",
    "        # labels with empty strings to make it easier. Otherwise, you might predict both 'none' and\n",
    "        # another label, which doesn't make any sense.\n",
    "        self.data[\"labels\"] = self.data[\"labels\"].str.replace(\"none\", \"\")\n",
    "\n",
    "        # For one-hot encoding, we need a list of all unique labels in the dataset and a map between\n",
    "        # labels and a unique ID. \n",
    "        \n",
    "        # TODO create self.labels: a list of every possible label in the dataset\n",
    "        # e.g., ['movie.starring.actor', 'movie.gross_revenue', ...]\n",
    "        # ======================================================================\n",
    "        self.labels = []\n",
    "        self.n_labels = len(self.labels)\n",
    "        \n",
    "        # TODO create self.label2id: a dictionary which maps the labels from self.labels (above) to a unique integer\n",
    "        # ======================================================================\n",
    "        self.label2id = None\n",
    "        \n",
    "        # Similarly, we often need to make a token vocabulary for encoding the input text. Note that\n",
    "        # this isn't necessary for ALL representations, but for some, you will find it useful. \n",
    "        # However, we are only creating the vocabulary from the training data. What happens if \n",
    "        # the test data has a token we haven't seen before? \n",
    "        # To combat this, we default to a particular token, usually something like <unk> ('unknown').\n",
    "        \n",
    "        # In the future, you will see datasets with hundreds of thousands of unique tokens. Normally,\n",
    "        # we only take the N most common tokens and replace everything else with <unk>. \n",
    "        # Otherwise, our models would be huge! For this dataset, it's not a problem, but you should know\n",
    "        # how to do it. \n",
    "        \n",
    "        # TODO create self.vocab: a dictionary which contains the `vocab_size` most common tokens in the text.\n",
    "        # Here's a hint - check out the `Counter` class from python's `collections` library.\n",
    "        # ======================================================================\n",
    "        self.vocab = {}\n",
    "        \n",
    "        # also, don't forget to include <unk> (unknown)\n",
    "        # TODO assign <unk> a unique ID. \n",
    "        # ======================================================================\n",
    "        self.vocab['<unk>'] = None\n",
    "        \n",
    "        self.vocab_size = vocab_size + 1 # plus 1 because <unk>\n",
    "    \n",
    "    def one_hot_encode_labels(self, labels: List[str]):\n",
    "        # For multi-label classification, we're going to one-hot encode our labels.\n",
    "        # This means that instead of having out data be pairs like:\n",
    "        #   {'input': ..., 'output': 2}\n",
    "        # We instead might have multiple correct classes, so we do something like\n",
    "        #   {'input': ..., 'output': [0, 0, 1, 0, ...]}\n",
    "        # where the output is a list with one element per possible label. Then, a 1 in position N means\n",
    "        # the label N is a correct answer.\n",
    "        \n",
    "        # We need to create such a list from the input to this function, `labels`, which is a list\n",
    "        # of labels that appear in a particular example. It might be, for example, \n",
    "        #   ['movie.starring.actor', 'movie.release_date']\n",
    "        # Good thing we have self.label2id! That should help us figure out which \n",
    "        # index corresponds to which label, so we can write our own function. \n",
    "        # Although...this is a very common thing to do in NLP,\n",
    "        # I'm sure it's available in a library somewhere (hint: sklearn). \n",
    "        \n",
    "        # TODO create encoded: a vector (np.array) which is a one-hot encoded \n",
    "        # representation of the input,`labels`. \n",
    "        # ======================================================================\n",
    "\n",
    "        encoded = labels  # do something to the labels!\n",
    "        return encoded\n",
    "    \n",
    "    def tokenize(self, text: str):\n",
    "        # Luckily, this dataset is already tokenized; that is, each token is separated by a single\n",
    "        # spce. Normally, text has punctuation, hyphenated words, paragraph breaks, etc.. which\n",
    "        # makes tokenization a more complicated problem. For now, just .split() is good enough.\n",
    "        return text.split()\n",
    "    \n",
    "    def encode_tokens(self, tokens: List[str]):\n",
    "        # Think about how you want to encode your tokens. One-hot encoding? Something else \n",
    "        # you've learned in 220 or 243?\n",
    "        # Whatever you decide, it's convenient if you are able to feed the output of this\n",
    "        # function directly into your model, like this:\n",
    "        #   >>> model(encode_tokens(['this', 'is', 'a', 'sentence']))\n",
    "        \n",
    "        # Note: that's only a suggestion, you don't have to feed this directly into the model.\n",
    "        # Feel free to set up your data/model pipeline as you see fit. \n",
    "        \n",
    "        # TODO create encoded: an encoded representation of `tokens`.\n",
    "        # ======================================================================\n",
    "\n",
    "        encoded = tokens  # do something to the tokens! \n",
    "        return encoded\n",
    "\n",
    "    def __len__(self):\n",
    "        # PyTorch expects every Dataset class to implement the __len__ function. \n",
    "        # Most of the time, it's very simple like this.\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        # TODO get the nth item of your data, and process it so it's ready to used in your model\n",
    "        # and for training.\n",
    "        # ======================================================================\n",
    "        \n",
    "        # Make sure the output of this function is either an np.array, a\n",
    "        # torch.Tensor, or a tuple of several of these. That way, pytorch\n",
    "        # can combine them into batches properly using its default collate_fn in the DataLoader.\n",
    "        # If you're using nn.Embedding, you will have to deal with padding,\n",
    "        # but I'll leave that for you :)\n",
    "\n",
    "        input_to_model = None\n",
    "        labels = None\n",
    "        return self.encode_tokens(input_to_model), self.one_hot_encode_labels(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1927e",
   "metadata": {
    "id": "a9a1927e"
   },
   "source": [
    "Now you can test out your code to make sure it's outputting what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d05102",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664552351873,
     "user": {
      "displayName": "Nilay Patel",
      "userId": "07518589520862055822"
     },
     "user_tz": 420
    },
    "id": "03d05102"
   },
   "outputs": [],
   "source": [
    "# Instantiate the data; you'll need to upload this file, or download the notebook and run it locally if you want this to work. \n",
    "dataset = MovieDataset('./data/hw1_train.csv')\n",
    "\n",
    "# A small batch size of 2 makes it easier to debug for printing. \n",
    "data_loader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d307c1f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1664552578595,
     "user": {
      "displayName": "Nilay Patel",
      "userId": "07518589520862055822"
     },
     "user_tz": 420
    },
    "id": "d307c1f2",
    "outputId": "17f338d4-92dd-49b0-9896-2389e0285ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64]) torch.Size([2, 20])\n",
      "torch.Size([2, 64]) torch.Size([2, 20])\n",
      "torch.Size([2, 64]) torch.Size([2, 20])\n",
      "torch.Size([2, 64]) torch.Size([2, 20])\n",
      "torch.Size([2, 64]) torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "# Zipping the dataloader with range(N) lets us only print the first N batches\n",
    "for _, batch in zip(range(5), data_loader):\n",
    "    # Do something here; maybe print the batch to see if it looks right to you?\n",
    "    print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b96523",
   "metadata": {
    "id": "27b96523"
   },
   "source": [
    "### Training\n",
    "\n",
    "You can check out the old Colab notebooks on the canvas which have some training loops you may find useful. Dive into Deep Learning should have some also\n",
    "\n",
    "I won't go too in-depth here, but remember, you are doing multi-label classification, which means you can't use regular cross-entropy loss.\n",
    "\n",
    "Instead, you'll need *binary* cross entropy. In pytorch, you'll find `BCELossWithLogits`. You can use it similarly, but make sure you pay attention to the inputs to the function in the documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Hmnf_1NpGlQS",
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1664553288881,
     "user": {
      "displayName": "Nilay Patel",
      "userId": "07518589520862055822"
     },
     "user_tz": 420
    },
    "id": "Hmnf_1NpGlQS"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "model = None # YourModelClass(...)\n",
    "optimizer = None # torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZZhYQSmGGiiB",
   "metadata": {
    "id": "ZZhYQSmGGiiB"
   },
   "outputs": [],
   "source": [
    "for epoch in n_epochs:\n",
    "    pbar = tqdm(data_loader) # tqdm is a progress bar\n",
    "    pbar.set_description(f'epoch: {epoch}')\n",
    "    for batch in pbar:  \n",
    "        # Three main things to do here:\n",
    "        # 1. run your model on the input\n",
    "        # 2. calculate loss of your output vs expected output\n",
    "        # 3. run backpropagation \n",
    "        # (optional) 4. Do some logging, e.g., print out loss, average loss over the epoch, etc. \n",
    "        # You could also calculate f1 on your training data, just for comparison.\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838b7cf",
   "metadata": {
    "id": "1838b7cf"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Your evaluation loop should look similar to your training loop, as you still have to loop through the items and apply your model to the input. The only difference is instead of calculating loss using the logits (the output of the model), you'll be converting the model output into your predictions. \n",
    "\n",
    "Remember, with regular multi-*class* classification, you do an `argmax` to find the index with the highest probability. \n",
    "\n",
    "However, with multi-*label* classification, this doesn't work - `argmax` only returns a single value, but there might be multiple (or none). \n",
    "\n",
    "Think about how you can decide which values in the model output correspond to a correct label and to an incorrect label. Here's a hint: first, use `torch.sigmoid` to normalize the model outputs to `[0, 1]`.\n",
    "\n",
    "#### F1 score\n",
    "Once you have your predictions, you have to calculate f1 score. You can do this manually...although this is a common thing to do in NLP, I'm sure there's a library that can do it for you (hint: sklearn, probably a million others). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f5ad66",
   "metadata": {
    "id": "49f5ad66"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "I hope this helps!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
