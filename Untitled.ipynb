{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda07979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "\n",
      "<class 'str'>\n",
      "1193\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "stopwords = English.Defaults.stop_words\n",
    "import numpy as np\n",
    "# print(type(stopwords))\n",
    "# print(stopwords)\n",
    "# item = 'lol'\n",
    "# print(item) if item in stopwords else print(\"ERROR!!\")\n",
    "# print(len(stopwords))\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "    \n",
    "data = pd.read_csv('Data/hw1_train-1.csv', index_col=0)\n",
    "data.columns = ['text', 'labels']\n",
    "data['labels'] = data['labels'].str.replace('none', '')\n",
    "data['labels'] = data['labels'].fillna('')\n",
    "\n",
    "print(data['labels'].loc[1452])\n",
    "print(type(data['labels'].loc[713]))\n",
    "\n",
    "# Creating a vocabulary\n",
    "vocab_size = 2_000\n",
    "all_tokens = []\n",
    "for text in data['text']:\n",
    "    tokens = tokenizer(text)\n",
    "    all_tokens.extend([i.text for i in tokens])\n",
    "# print(all_tokens)    \n",
    "\n",
    "# Decoder\n",
    "decoder = dict(enumerate(all_tokens))\n",
    "# print(decoder)\n",
    "\n",
    "# Encoder\n",
    "encoder = {token: idx for idx, token in decoder.items()}\n",
    "print(len(encoder))\n",
    "encoder['<unk>'] = len(encoder)\n",
    "vocab = encoder\n",
    "# print(vocab)  \n",
    "default = vocab['<unk>']\n",
    "\n",
    "def tokenize(text):\n",
    "    return [i.text for i in tokenizer(text)]\n",
    "   \n",
    "def stopword_removal(text):\n",
    "    corpus_row = []\n",
    "    for item in tokenize(text):\n",
    "        print(item)\n",
    "        if item not in stopwords:\n",
    "            corpus_row.append(item)\n",
    "    corpus_row = ' '.join(corpus_row)          \n",
    "    return corpus_row\n",
    "      \n",
    "# print(data.loc[100])\n",
    "# print(tokenize(data['text'].loc[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558771af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data: str):\n",
    "    # Convert text to lowercase\n",
    "    data = data.lower()\n",
    "    # Removing all punctuation\n",
    "    data = re.sub(r'[^\\w\\s]', '', data)\n",
    "    # Removing links\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    # Removing special characters and numbers\n",
    "    data = re.sub(r'[^A-Za-z0-9]+', ' ', data)\n",
    "    # Removing single characters\n",
    "    data = re.sub(r's+[a-zA-Z]s+', '', data)\n",
    "    # Removing multi-spaces by a single space\n",
    "    datav = re.sub(r'\\s+', '', data)\n",
    "    # Removing Stopwords\n",
    "    # stopword_removal(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514cf7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find the male actor from the movie born free\n",
      "find the male actor from the movie born free\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(data['text'].loc[86])\n",
    "print(preprocess_text(data['text'].loc[86]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74dedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'the', 'male', 'actor', 'from', 'the', 'movie', 'born', 'free']\n",
      "find\n",
      "the\n",
      "male\n",
      "actor\n",
      "from\n",
      "the\n",
      "movie\n",
      "born\n",
      "free\n",
      "find male actor movie born free\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(data['text'].loc[86]))\n",
    "print(stopword_removal(data['text'].loc[86]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbc6d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14309, 14506,  1034, 12540, 12049, 14506, 14456,  1144,   629])\n"
     ]
    }
   ],
   "source": [
    "def encode_tokens(tokens):\n",
    "    encoded = [vocab.get(token, default) for token in tokens]\n",
    "    return torch.tensor(encoded)\n",
    "\n",
    "print(encode_tokens(tokenize(data['text'].loc[86])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b4e06ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie.subjects']\n",
      " ['movie.locations']\n",
      " ['movie.genre']\n",
      " ['movie.rating']\n",
      " ['actor.gender']\n",
      " ['movie.production_companies']\n",
      " ['gr.amount']\n",
      " ['movie.estimated_budget']\n",
      " ['movie.starring.actor']\n",
      " ['movie.country']\n",
      " ['movie.initial_release_date']\n",
      " ['movie.gross_revenue']\n",
      " ['movie.language']\n",
      " ['movie.produced_by']\n",
      " ['movie.directed_by']\n",
      " ['person.date_of_birth']\n",
      " ['movie.music']\n",
      " ['movie.starring.character']]\n",
      "{0: 'actor.gender', 1: 'gr.amount', 2: 'movie.country', 3: 'movie.directed_by', 4: 'movie.estimated_budget', 5: 'movie.genre', 6: 'movie.gross_revenue', 7: 'movie.initial_release_date', 8: 'movie.language', 9: 'movie.locations', 10: 'movie.music', 11: 'movie.produced_by', 12: 'movie.production_companies', 13: 'movie.rating', 14: 'movie.starring.actor', 15: 'movie.starring.character', 16: 'movie.subjects', 17: 'person.date_of_birth'}\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for index, row in data.iterrows():\n",
    "    temp = row['labels'].split()\n",
    "    for item in temp:\n",
    "        label_set.add(item)\n",
    "# print(label_set)\n",
    "# print(len(label_set))\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "A = np.array(list(label_set))\n",
    "list_of_labels = np.reshape(A, (-1, 1))\n",
    "print(list_of_labels)\n",
    "# print(list_of_labels.shape)    \n",
    "# print(type(list_of_labels))\n",
    "\n",
    "mlb.fit(list_of_labels)\n",
    "# print(mlb.classes_)\n",
    "# print(type(mlb.classes_))\n",
    "# print(type(mlb.classes_))\n",
    "# print(len(mlb.classes_))\n",
    "\n",
    "# Fit Transform\n",
    "label_to_id = {}\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    label_to_id[idx] = label\n",
    "print(label_to_id)\n",
    "\n",
    "label_row = data['labels'].loc[17]\n",
    "label_row = label_row.split()\n",
    "label_row_np = np.array(list(label_row))\n",
    "# print(label_row_np)\n",
    "label_row_matrix = np.reshape(label_row_np, (1, -1))\n",
    "# print(label_row_matrix)\n",
    "# print(label_row_matrix.shape)\n",
    "\n",
    "encoded = mlb.transform(label_row_matrix)\n",
    "print(encoded)\n",
    "print(type(encoded))\n",
    "encoded_tensor = torch.from_numpy(encoded)\n",
    "print(encoded_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
