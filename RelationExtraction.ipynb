{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "stopwords = English.Defaults.stop_words\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Set device = CUDA if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV File into Pandas DataFrame\n",
    "df = pd.read_csv('Data/hw1_train-1.csv', index_col=0)\n",
    "df.columns = ['text', 'labels']\n",
    "df['labels'] = df['labels'].str.replace('none', '')\n",
    "df['labels'] = df['labels'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: \n",
      "                                            text                        labels\n",
      "ID                                                                            \n",
      "909               can you show me movies spanish  movie.country movie.language\n",
      "820                         in search for movies                              \n",
      "1185  are there any g movies out there right now                  movie.rating\n",
      "1971   show me more information about will smith                              \n",
      "564                          who directed batman             movie.directed_by\n",
      "Val Data: \n",
      "                                               text  \\\n",
      "ID                                                    \n",
      "2154          show me the producer of the godfather   \n",
      "563   can you tell me who is the director of batman   \n",
      "789      get details of original finding nemo movie   \n",
      "1325                             list french movies   \n",
      "570    who directed the movie the thing called love   \n",
      "\n",
      "                            labels  \n",
      "ID                                  \n",
      "2154             movie.produced_by  \n",
      "563              movie.directed_by  \n",
      "789     movie.initial_release_date  \n",
      "1325  movie.country movie.language  \n",
      "570              movie.directed_by  \n",
      "Length -> Train Data: 1734\n",
      "Length -> Val Data: 578\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Validation set\n",
    "train_data, val_data = train_test_split(df,\n",
    "                                        random_state=0, \n",
    "                                        test_size=0.25, \n",
    "                                        shuffle=True)\n",
    "print(\"Train Data: \")\n",
    "print(train_data.head())\n",
    "print(\"Val Data: \")\n",
    "print(val_data.head())\n",
    "print('Length -> Train Data: ' + str(len(train_data)))\n",
    "print('Length -> Val Data: ' + str(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vocabulary\n",
    "vocab_size = 2_000\n",
    "all_tokens = []\n",
    "for text in df['text']:\n",
    "    tokens = tokenizer(text)\n",
    "    all_tokens.extend([i.text for i in tokens])\n",
    "# print(all_tokens)    \n",
    "\n",
    "# Decoder\n",
    "decoder = dict(enumerate(all_tokens))\n",
    "# print(decoder)\n",
    "\n",
    "# Encoder\n",
    "encoder = {token: idx for idx, token in decoder.items()}\n",
    "# print(len(encoder))\n",
    "encoder['<unk>'] = len(encoder)\n",
    "vocab = encoder\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of Labels\n",
    "label_set = set()\n",
    "for index, row in df.iterrows():\n",
    "    temp = row['labels'].split()\n",
    "    for item in temp:\n",
    "        label_set.add(item)      \n",
    "\n",
    "label_list = np.reshape(np.array(list(label_set)), \n",
    "                            (-1, 1))\n",
    "# print(label_list)\n",
    "# print(label_list.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'actor.gender', 1: 'gr.amount', 2: 'movie.country', 3: 'movie.directed_by', 4: 'movie.estimated_budget', 5: 'movie.genre', 6: 'movie.gross_revenue', 7: 'movie.initial_release_date', 8: 'movie.language', 9: 'movie.locations', 10: 'movie.music', 11: 'movie.produced_by', 12: 'movie.production_companies', 13: 'movie.rating', 14: 'movie.starring.actor', 15: 'movie.starring.character', 16: 'movie.subjects', 17: 'person.date_of_birth'}\n"
     ]
    }
   ],
   "source": [
    "# Fit MLB to Label List\n",
    "mlb.fit(label_list)\n",
    "# print(mlb.classes_)\n",
    "# print(len(mlb.classes_))\n",
    "\n",
    "# Enumerate classes for Labels        \n",
    "id_to_label = {}\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    id_to_label[idx] = label\n",
    "print(id_to_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "vectorizer.fit(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "s2OsKlUseX_0"
   },
   "outputs": [],
   "source": [
    "# Relation Extraction Class for DataLoader\n",
    "class RelationExtractionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 vocab):\n",
    "        self.data = data\n",
    "        self.text = self.data['text']\n",
    "        self.labels = self.data['labels']\n",
    "        self.vocab = vocab\n",
    "        self.default = self.vocab['<unk>']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, \n",
    "                    idx: int):\n",
    "        text = self.text.iloc[idx]\n",
    "        text = self.preprocess_text(self.text.iloc[idx])\n",
    "        label = self.labels.iloc[idx].split()\n",
    "        return self.encode_text(text), self.encode_label(label)\n",
    "\n",
    "    def encode_text(self, \n",
    "                    text: str):\n",
    "        text_list = []\n",
    "        text_list.append(text)\n",
    "        # Return tensor of encoded text using TF-IDF\n",
    "        encoded = vectorizer.transform(text_list)\n",
    "        encoded_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(encoded)).float()\n",
    "        return encoded_tensor\n",
    "        \n",
    "    def encode_label(self, \n",
    "                     label: str):\n",
    "        # Convert label into NumPy matrix\n",
    "        label_array = np.array(label)\n",
    "        label_matrix = np.reshape(label_array, \n",
    "                                  (1, -1))\n",
    "        # Return tensor of encoded label using Multi-Label Binarizer\n",
    "        encoded = mlb.transform(label_matrix)\n",
    "        encoded_label = torch.from_numpy(encoded)\n",
    "        return encoded_label\n",
    "         \n",
    "    def tokenize(self, \n",
    "                 text: str):\n",
    "        return [i.text for i in tokenizer(text)]\n",
    "\n",
    "    def stopword_removal(self,\n",
    "                         text: str):\n",
    "        corpus = []\n",
    "        for item in self.tokenize(text):\n",
    "            if item not in stopwords:\n",
    "                corpus.append(item)\n",
    "        corpus = ' '.join(corpus)          \n",
    "        return corpus\n",
    "\n",
    "    def preprocess_text(self, \n",
    "                        text: str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Removing text contractions\n",
    "        text = re.sub(r\"there's\", \"there is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"who's\", \"who is\", text)\n",
    "        text = re.sub(r\"you're\", \"you are\", text)\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"i'd\", \"i would\", text)\n",
    "        text = re.sub(r\"ain't\", \"am not\", text)\n",
    "        text = re.sub(r\"don't\", \"donot\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"could't\", \"could not\", text)\n",
    "        text = re.sub(r\"should'nt\", \"should not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        # Removing all punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Removing links\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Removing special characters and numbers\n",
    "        text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "        # Removing single characters\n",
    "        text = re.sub(r's+[a-zA-Z]s+', '', text)\n",
    "        # Removing stopwords\n",
    "        text = self.stopword_removal(text)\n",
    "        # Replacing multi-spaces by a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation Extraction Object for DataLoader\n",
    "train_ds = RelationExtractionDataset(train_data, vocab)\n",
    "val_ds = RelationExtractionDataset(val_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PyTorch Data Loader\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        hidden_dim, \n",
    "        output_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=981, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=200, out_features=18, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "input_features_dim = train_ds[0][0].shape[1]\n",
    "model = MLP(input_features_dim, \n",
    "            200, \n",
    "            18).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Function\n",
    "def train(loader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        y = y.squeeze()\n",
    "        y = y.to(torch.float32)\n",
    "        \n",
    "        y_pred = y_pred.squeeze()\n",
    "        y_pred = y_pred.to(torch.float32)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        # Calculate gradients for w/b\n",
    "        loss.backward()  \n",
    "        # Update weights according to optimizer rules\n",
    "        optimizer.step()  \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "# Model Evaluate Function\n",
    "def evaluate(loader, \n",
    "             model, \n",
    "             loss_fn, \n",
    "             score_fn):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for x, y in tqdm(loader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        y = y.squeeze()\n",
    "        y = y.to(torch.float32)\n",
    "        \n",
    "        y_pred = y_pred.squeeze()\n",
    "        y_pred = y_pred.to(torch.float32)\n",
    "        y_pred = torch.round(y_pred)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        y = y.detach().numpy()\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        \n",
    "        predictions.extend(y_pred)\n",
    "        labels.extend(y)\n",
    "    \n",
    "#     print(\"Y: \")\n",
    "#     print(y)\n",
    "#     print(\"Y_PRED: \")\n",
    "#     print(y_pred)\n",
    "    \n",
    "    score = score_fn(labels, predictions, average='weighted')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1324.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6672, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2877.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9145841911753578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1359.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6650, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2949.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9187624746063294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1358.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6627, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2937.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9210888695859241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1354.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6605, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2952.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209486085727029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1352.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6583, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2918.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1332.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6561, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2795.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1361.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6538, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2962.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1367.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6516, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2913.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1360.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6494, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2952.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1734/1734 [00:01<00:00, 1354.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  tensor(0.6472, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 2945.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy:  0.9209966335603219\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "score_fn = f1_score\n",
    "\n",
    "n_epochs = 10\n",
    "best_acc = 0\n",
    "PATH = f'best-model.pt'\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = train(train_loader, \n",
    "                     model, \n",
    "                     optimizer, \n",
    "                     loss_fn)\n",
    "    print('Train Loss: ', avg_loss)\n",
    "    accuracy = evaluate(val_loader, \n",
    "                        model, \n",
    "                        loss_fn, \n",
    "                        score_fn)\n",
    "    print('Val Accuracy: ', accuracy)\n",
    "    if accuracy > best_acc and accuracy > 0.7:\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=981, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=200, out_features=18, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and Load the Model\n",
    "saved_model = MLP(input_features_dim, \n",
    "            200, \n",
    "            18).to(device)\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(new_model(mystery_iris))\n",
    "    print()\n",
    "    print(labels[new_model(mystery_iris).argmax()])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
